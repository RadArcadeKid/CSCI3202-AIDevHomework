{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3202: Intro to AI - Fall 2020 Practicum 2\n",
    "\n",
    "## Your name: Jacob (Jake) Henson - 105963531\n",
    "\n",
    "#### Collaborator's name (optional): N/A\n",
    "\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1: Search](#p1) | [Problem 2: MDP](#p2) | [Problem 3: Q_Learn](#bot)\n",
    "\n",
    "---\n",
    "\n",
    "This practicum is due on Canvas by **10:00 PM on Saturday December 12**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  \n",
    "\n",
    "**Here are the rules:** \n",
    "\n",
    "1. All work, code and analysis, must be your own. \n",
    "2. You may use your course notes, posted lecture slides, textbooks, in-class notebooks, and homework solutions as resources.  You may also search online for answers to general knowledge questions like the form of a probability distributions or how to perform a particular operation in Python/Pandas. \n",
    "3. This is meant to be like a coding portion of your final exam. So, the instructional team will be much less helpful than we typically are with homework. For example, we will not check answers, help debug your code, and so on.\n",
    "4. If something is left open-ended, it is because we want to see how you approach the kinds of problems you will encounter in the wild, where it will not always be clear what sort of tests/methods should be applied. Feel free to ask clarifying questions though.\n",
    "5. You may **NOT** post to message boards or other online resources asking for help.  If you have a question for us, post it as a **PRIVATE** message on Piazza.  If we decide that the question is appropriate for the entire class, then we will add it to a Practicum clarifications thread. \n",
    "6. You may re-use your code or code given from in-class solutions (for e.g. Astar, MDP), but you **must cite** in comments any regions of code that were not created anew for this practicum.\n",
    "7. You may collaborate with **exactly one** of your classmates.  You must each submit your own assignments and write your own code, and may only collaborate on ideas, psuedocode, etc.  If you choose to collaborate with another student in the class, list their name under yours above.\n",
    "8. In short, **your work must be your own**. It really is that simple.\n",
    "\n",
    "Violation of the above rules will result in an immediate academic sanction (*at the very least*, you will receive a 0 on this practicum or an F in the course, depending on severity), and a trip to the Honor Code Council.\n",
    "\n",
    "**By submitting this assignment, you agree to abide by the rules given above.**\n",
    "\n",
    "***\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- You may not use late days on the practicums nor can you drop your practicum grades.\n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. \n",
    "- This should go without saying, but... For any question that asks you to calculate something, you **must show all work to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# added packages\n",
    "import heapq\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider the map of the area to the west of the Engineering Center given below, with a fairly coarse Cartesian grid superimposed.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/engineering_center_grid_zoom.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "The green square at $(x,y)=(1,15)$ is the starting location, and you would like to walk from there to the yellow square at $(25,9)$. The filled-in blue squares are obstacles, and you cannot walk through those locations.  You also cannot walk outside of this grid.\n",
    "\n",
    "Legal moves in the North/South/East/West directions have a step cost of 1. Moves in the diagonal direction (for example, from $(1,15)$ to $(2,14)$) are allowed, but they have a step cost of $\\sqrt{2}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some tuples defining the wall and state locations, for your convenience\n",
    "\n",
    "walls = [(1,y) for y in range(2,15)] + [(2,y) for y in range(3,14)] + [(3,y) for y in range(4,13)] + \\\n",
    "        [(4,y) for y in range(5,12)] + [(x,1) for x in range(5,24)] + [(10,y) for y in range(9,13)] + \\\n",
    "        [(x,y) for x in range(11,14) for y in range(9,15)] + [(14,y) for y in range(11,15)] + \\\n",
    "        [(x,y) for x in range(21,26) for y in range(11,17)] + \\\n",
    "        [(x,y) for x in [0,26] for y in range(0,18)] + [(x,y) for x in range(0,26) for y in [0,17]]\n",
    "        \n",
    "states = [(x,y) for x in range(1,26) for y in range(1,17)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "<a id='p1'></a>\n",
    "\n",
    "\n",
    "---\n",
    "## [40 pts] Part 1:  Route-finding\n",
    "In this problem, our goal is to find the path from the green to yellow squares with the **shortest total path length**.\n",
    "\n",
    "Of course, you can probably do this problem (and likely have to some degree, in your head) without a search algorithm. But that will hopefully provide a useful \"sanity check\" for your answer.\n",
    "\n",
    "#### Part A\n",
    "Write a function `adjacent_states(state)`:\n",
    "* takes a single argument `state`, which is a tuple representing a valid state in this state space\n",
    "* returns in some form the states reachable from `state` and the step costs. How exactly you do this is up to you.\n",
    "\n",
    "Print to the screen the output for `adjacent_states((1,15))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of (2,2) are: {(3, 3): 1.4142135623730951, (3, 2): 1, (3, 1): 1.4142135623730951, (2, 1): 1, (1, 1): 1.4142135623730951}\n",
      "Neighbors of (1,1) are: {(3, 3): 1.4142135623730951, (3, 2): 1, (3, 1): 1.4142135623730951, (2, 1): 1, (1, 1): 1.4142135623730951}\n",
      "Neighbors of (25,5) are: {(3, 3): 1.4142135623730951, (3, 2): 1, (3, 1): 1.4142135623730951, (2, 1): 1, (1, 1): 1.4142135623730951}\n"
     ]
    }
   ],
   "source": [
    "#redefine number of total rows and columns for easy access \n",
    "nrows = max(states)[1]\n",
    "ncols = max(states)[0]\n",
    "#print(nrows, ncols)\n",
    "\n",
    "def adjacent_states(state):\n",
    "    #check 8 possible combinations of movement \n",
    "    x = state[0]\n",
    "    y = state[1]\n",
    "    neighbors = {} #empty dictionary to store neighboring values \n",
    "    \n",
    "    \n",
    "    #check if wall, failsafe \n",
    "    if((x,y) in walls):\n",
    "        return {};\n",
    "    \n",
    "    #North\n",
    "    if(y+1 <= nrows and (x, y+1) not in walls):\n",
    "        neighbors.update({(x, y+1): 1}) #append N, pathcost 1 \n",
    "    #NorthEast\n",
    "    if((x+1, y+1) <= (nrows, ncols) and (x+1, y+1) not in walls):\n",
    "        neighbors.update({(x+1, y+1): np.sqrt(2)}) #append NE, pathcost sqrt(2)\n",
    "    #East\n",
    "    if(x+1 <= ncols and (x+1, y) not in walls):\n",
    "        neighbors.update({(x+1, y): 1}) #append E, pathcost 1\n",
    "    #SouthEast\n",
    "    if(x+1 < ncols and y-1 > 0 and (x+1, y-1) not in walls):\n",
    "        neighbors.update({(x+1, y-1): np.sqrt(2)}) #append SE, pathcost sqrt(2)\n",
    "    #South\n",
    "    if(y-1 > 0 and (x, y-1) not in walls):\n",
    "        neighbors.update({(x, y-1): 1}) #append S, pathcost 1      \n",
    "    #SouthWest\n",
    "    if((x-1, y-1) > (0, 0) and (x-1, y-1) not in walls):\n",
    "        neighbors.update({(x-1,y-1): np.sqrt(2)}) #append SW, pathcost, pathcost sqrt(2)\n",
    "    #West\n",
    "    if(x-1 > 0 and (x-1, y) not in walls):\n",
    "        neighbors.update({(x-1, y): 1}) #append W, pathcost 1\n",
    "    #NorthWest\n",
    "    if(x-1 > 0 and y+1 <= nrows and (x-1, y+1) not in walls):\n",
    "        neighbors.update({(x-1,y+1): np.sqrt(2)}) #append NW, pathcost, pathcost sqrt(2)    \n",
    "    \n",
    "    return neighbors\n",
    "\n",
    "\n",
    "print(\"Neighbors of (2,2) are:\", adjacent_states((2,2)))\n",
    "print(\"Neighbors of (1,1) are:\", adjacent_states((2,2)))\n",
    "print(\"Neighbors of (25,5) are:\", adjacent_states((2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "Three candidate heuristic functions might be:\n",
    "1. `heuristic_cols(state, goal)` = number of columns between the argument `state` and the `goal`\n",
    "1. `heuristic_rows(state, goal)` = number of rows between the argument `state` and the `goal`\n",
    "1. `heuristic_eucl(state, goal)` = Euclidean distance between the argument `state` and the `goal`\n",
    "\n",
    "Write a function `heuristic_max(state, goal)` that returns the maximum of all three of these heuristic functions for a given `state` and `goal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretty simple. I hope this should be an absolute value \n",
    "def heuristic_cols(state, goal):\n",
    "    return abs(goal[0] - state[0])\n",
    "\n",
    "#pretty simple. I hope this should be an absolute value \n",
    "def heuristic_rows(state, goal):\n",
    "    return abs(goal[1] - state[1])\n",
    "\n",
    "#returns euclid distance. unsure if it should be abs(goal[0]-state[0]) but this seems to work...  \n",
    "def heuristic_eucl(state, goal):\n",
    "    #euclid distance == sqrt((x1-x2)^2 + (y1,y2)^2)\n",
    "    return np.sqrt((goal[0] - state[0])**2 + (goal[1] - state[1])**2)\n",
    "\n",
    "#get the max of the three heuristics, return it \n",
    "def heuristic_max(state, goal):\n",
    "    h_cols = heuristic_cols(state, goal)\n",
    "    h_rows = heuristic_rows(state, goal)\n",
    "    h_eucl = heuristic_eucl(state, goal)\n",
    "                            \n",
    "    return max(h_cols, h_rows, h_eucl)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "Is the Manhattan distance an admissible heuristic function for this problem?  Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A heuristic is only admissible if it NEVER overestimates the actual path cost. In this case, Manhattan distance heuristic overestimates the pathcost cost, because it does not account for a pathcost of moving diagonally as sqrt(2) - this means Manhattan distance is NOT an admissible heuristic. It implies diagonal movement is more costly than it actually is. As an example, a pathcost between (17,5) and (25,9) produces a heuristic cost of 12 from manhattan, despite the shortest path being ~10.**\n",
    "\n",
    "*Manhattan Distance:*   $\\lvert 17-25\\rvert + \\lvert 5-9\\rvert    = 12$\n",
    "\n",
    "Whereas the shortest actual path cost from (17, 5) to (25,9) is around 10:\n",
    "- (17, 5), (18, 5), (19, 5), (20,5), (21,5) (22,6), (23,7), (24, 8), (25,9)\n",
    "-      $+1        +1       + 1      +1  +\\sqrt2 +\\sqrt2 +\\sqrt2  +\\sqrt2  = 9.656...$\n",
    "\n",
    "\n",
    "\n",
    "$h(n) \\nleq h^*(n)$\n",
    "\n",
    "$12 \\nleq 9.656$ \n",
    "\n",
    "\n",
    "**This proves Manhattan distance is A NONADMISSABLE HEURISTIC**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "Use A\\* search and the `heuristic_max` heuristic to find the shortest path from the initial state at $(1,15)$ to the goal state at $(25,9)$. Your search **should not** build up the entire state space graph in memory. Instead, use the `adjacent_states` function from Part A, similarly to the 8-tile problem from Homework 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.313708498984763\n",
      "[(1, 15), (2, 14), (3, 13), (4, 12), (5, 11), (6, 10), (7, 9), (8, 9), (9, 9), (10, 8), (11, 8), (12, 8), (13, 8), (14, 9), (15, 9), (16, 9), (17, 9), (18, 9), (19, 9), (20, 9), (21, 9), (22, 9), (23, 9), (24, 9), (25, 9)]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "#step cost is my own function, simply returns the step_cost using the adjacent_states function from earlier \n",
    "def step_cost(current, previous):\n",
    "    neighbors = adjacent_states(previous);\n",
    "    return neighbors[current]\n",
    "    \n",
    "#PATH and PATHCOST functions borrowed from previous labs, used for simplicity    \n",
    "def path(previous, s): \n",
    "    if s is None:\n",
    "        return []\n",
    "    else:\n",
    "        return path(previous, previous[s])+[s]\n",
    "    \n",
    "    \n",
    "    \n",
    "def pathcost(path):\n",
    "    cost = 0\n",
    "    for s in range(len(path)-1):\n",
    "        cost += step_cost(path[s], path[s+1])\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "#frontier_PQ from previous homeworks \n",
    "#taken here because my implementation was clunky at best \n",
    "class Frontier_PQ:\n",
    "    def __init__(self, start, cost):\n",
    "        self.states = {}\n",
    "        self.q = []\n",
    "        self.add(start, cost)\n",
    "        \n",
    "    def add(self, state, cost):\n",
    "        heapq.heappush(self.q, (cost, state))\n",
    "        self.states[state] = cost\n",
    "\n",
    "    def pop(self):\n",
    "        (cost, state) = heapq.heappop(self.q)  # get cost of getting to explored state\n",
    "        self.states.pop(state)    # and remove from frontier\n",
    "        return (cost, state)\n",
    "\n",
    "    def replace(self, state, cost):\n",
    "        self.states[state] = cost\n",
    "        for i, (oldcost, oldstate) in enumerate(self.q):\n",
    "            if oldstate==state and oldcost > cost:\n",
    "                self.q[i] = (cost, state)\n",
    "                heapq._siftdown(self.q, 0, i) # now i is posisbly out of order; restore\n",
    "        return\n",
    "        \n",
    "        \n",
    "#reused a* from the previous hw's solutions because this was the most simple appraoch \n",
    "def astar_path(start, goal, states, heuristic):\n",
    "    frontier_set = Frontier_PQ(start, 0)\n",
    "    previous = {start : None}\n",
    "    explored = {} #empty explored, haven't explored anything yet \n",
    "    \n",
    "    while frontier_set:\n",
    "        s = frontier_set.pop()\n",
    "        if s[1] == goal:\n",
    "            return path(previous, s[1])\n",
    "        \n",
    "        explored[s[1]] = len(path(previous, s[1]))-1\n",
    "        current_neighbors = adjacent_states(s[1])\n",
    "        \n",
    "        for neighbor in current_neighbors:\n",
    "            newcost = explored[s[1]] + heuristic(neighbor, goal) +1\n",
    "            if (neighbor not in explored) and (neighbor not in frontier_set.states):\n",
    "                frontier_set.add(neighbor, newcost)\n",
    "                previous[neighbor] = s[1]\n",
    "            elif (neighbor in frontier_set.states) and (frontier_set.states[neighbor] > newcost):\n",
    "                frontier_set.replace(neighbor, newcost)\n",
    "                previous[neighbor] = s[1]\n",
    "             \n",
    "    \n",
    "#yeah, i know i probably shouldn't make these globals... \n",
    "start = (1,15)\n",
    "goal = (25, 9)\n",
    "\n",
    "#passing in heuristic_max here from earlier, with the state matrix, and above start and goal \n",
    "solution = astar_path(start, goal, states, heuristic_max)\n",
    "#pathcost(solution) is just for my own sanity \n",
    "print(pathcost(solution))\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "Make a figure depicting the optimal route from the initial state to the goal, similarly to how you depicted the maze solution in Homework 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#general numpy help with the grid function from: https://stackoverflow.com/questions/43971138/python-plotting-colored-grid-based-on-values\n",
    "\n",
    "\n",
    "#create new numpy array with zeros\n",
    "grid = np.zeros(shape=(nrows,ncols))\n",
    "\n",
    "#check if these are walls, convert them to 1\n",
    "for y in range(1, nrows):\n",
    "    for x in range(1, ncols):\n",
    "        if((x,y) in walls):\n",
    "            grid[nrows-y,x-1] = 1 \n",
    "\n",
    "\n",
    "#add last final bits of wall   \n",
    "grid[5, 24] = 1\n",
    "grid[4, 24] = 1\n",
    "grid[3, 24] = 1\n",
    "grid[2, 24] = 1\n",
    "grid[1, 24] = 1\n",
    "grid[0, 24] = 1\n",
    "grid[0, 23] = 1\n",
    "grid[0, 22] = 1\n",
    "grid[0, 21] = 1\n",
    "grid[0, 20] = 1\n",
    "\n",
    "    \n",
    "#get solution path: \n",
    "for(x, y) in solution: \n",
    "    grid[nrows-y, x-1] = 2 \n",
    "    \n",
    "    \n",
    "#get goal space \n",
    "grid[nrows-9, 25-1] = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create discrete colormap\n",
    "cmap = colors.ListedColormap(['white', 'darkblue', 'lime', 'orange'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,9))\n",
    "ax.imshow(grid, cmap=cmap)\n",
    "\n",
    "  # draw gridlines\n",
    "ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=1)\n",
    "ax.set_xticks(np.arange(0.51, 25, 1));\n",
    "ax.set_xticklabels([]);\n",
    "ax.set_yticks(np.arange(0.47, 16, 1));\n",
    "ax.set_yticklabels([]);\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "<a id='p2'></a>\n",
    "\n",
    "\n",
    "---\n",
    "## [40 pts] Part 2:  States and Values\n",
    "\n",
    "Winter has come, and now the area west of Engineering is icy and slippery.  As a result, there's a risk that we don't end up in the tile that we intend to move to!  In particular, if we have $k$ available actions in state $n$, the probably that we move to the state $s'$ we intend to is 75\\%, and the remaining 0.25 probability is spread equally likely across all of the other adjacent (N/S/E/W/NW/NE/SE/SW) non-wall states.\n",
    "\n",
    "In this problem, our goal is to create a policy for an agent walking in the given space west of Engineering.  Again, the goal of the agent is to navigate from start to finish, but now we want a policy for each and every location on the map.\n",
    "\n",
    "This time, however, we're going to add the same type of randomness that our process in homework 4 had.\n",
    "\n",
    "Because the state space is fully observable, we should be able to implement this as a Markov Decision process.\n",
    "\n",
    "\n",
    "#### Part A:\n",
    "\n",
    "Write the necessary functions to create *either* a **value iteration** or **policy iteration** scheme to solve for the MDP.  If you wish to follow the schema for homework 4, you may want to create an `MDP` class, with methods:\n",
    "\n",
    "- `actions`, given by the valid successor states $s'$ from all actions $a$ in state $s$ in your adjacent_states from part 1.\n",
    "- `rewards`, given by a significant positive reward for the goal state (e.g. 10) and a small negative reward for spending a long time in the system (e.g. non-wall reward of -0.01).\n",
    "- `result`, which returns the successor state $s'$ of an *actual* movement $a$ from state $s$.\n",
    "- `transition`, which returns the probability of an actual successor $s'$ given action $a$ from state $s$ using the 75\\%-25% split above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE - some of this class is from the HW4 solutions, some of it was from my own work on HW4\n",
    "#Either way, it's been modified to work here properly \n",
    "class MDP:\n",
    "    def __init__(self, nrow, ncol, terminal, default_reward, discount):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        states = [(x,y) for x in range(1,ncol+1) for y in range(1,nrow+1)]\n",
    "        self.states = states\n",
    "        self.terminal_states = terminal\n",
    "        self.default_reward = default_reward\n",
    "        self.df = discount\n",
    "\n",
    "    def actions(self, state):\n",
    "        if state in self.terminal_states:\n",
    "            return [None]\n",
    "        else:\n",
    "            moves = []\n",
    "            y = state[1]\n",
    "            x = state[0]\n",
    "            if((x, y) in walls):\n",
    "                return [None]\n",
    "            #as per usual, should work with adjacent states here\n",
    "            #basically the opposite of the get_neighbors\n",
    "            #i'm sure there's a faster way to do this, but eh\n",
    "            #note that this will auto-filter for walls since adjacent neighbors won't return a wall space \n",
    "            for neighbor in adjacent_states(state):\n",
    "                if neighbor not in walls: \n",
    "                    #North\n",
    "                    if(neighbor == (x, y+1)):\n",
    "                        moves.append('N')\n",
    "                    #NorthEast\n",
    "                    if(neighbor == (x+1, y+1)):\n",
    "                        moves.append('NE')\n",
    "                    #East\n",
    "                    if(neighbor == (x+1, y)):\n",
    "                        moves.append('E')\n",
    "                    #SouthEast\n",
    "                    if(neighbor == (x+1, y-1)):\n",
    "                        moves.append('SE')\n",
    "                    #South\n",
    "                    if(neighbor == (x, y-1)):\n",
    "                        moves.append('S')\n",
    "                    #SouthWest\n",
    "                    if(neighbor == (x-1, y-1)):\n",
    "                        moves.append('SW')\n",
    "                    #West\n",
    "                    if(neighbor == (x-1, y)):\n",
    "                        moves.append('W')\n",
    "                    #NorthWest\n",
    "                    if(neighbor == (x-1, y+1)):\n",
    "                        moves.append('NW')                \n",
    "\n",
    "            return moves \n",
    "        \n",
    "        \n",
    "        \n",
    "    def reward(self, state):\n",
    "        return self.terminal_states[state] if state in self.terminal_states.keys() else self.default_reward\n",
    "        \n",
    "    def result(self, state, action):\n",
    "        assert action in self.actions(state), 'Error: action needs to be available in that state'\n",
    "        assert state in self.states, 'Error: invalid state'\n",
    "    \n",
    "        if action==None:\n",
    "            return state\n",
    "    \n",
    "        if action=='N': \n",
    "            new_state = (state[0], state[1]+1)\n",
    "        elif action == 'NE':\n",
    "            new_state = (state[0]+1, state[1]+1)\n",
    "        elif action=='S':\n",
    "            new_state = (state[0], state[1]-1)\n",
    "        elif action=='SE':\n",
    "            new_state = (state[0]+1, state[1]-1)\n",
    "        elif action=='SW':\n",
    "            new_state = (state[0]-1, state[1]-1)\n",
    "        elif action=='E':\n",
    "            new_state = (state[0]+1, state[1])\n",
    "        elif action=='W':\n",
    "            new_state = (state[0]-1, state[1])\n",
    "        elif action == 'NW':\n",
    "            new_state = (state[0]-1, state[1]+1)\n",
    "        \n",
    "        return new_state\n",
    "                \n",
    "    def transition(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0, state)]\n",
    "        else:\n",
    "            avaliable_actions = self.actions(state)\n",
    "            total_actions = len(avaliable_actions)\n",
    "\n",
    "            #determine probability of success and fail states \n",
    "            P_success_state = 0.75; \n",
    "            P_others = 0.25 / total_actions\n",
    "        \n",
    "            #set up previous states\n",
    "            # a list of tupes from current to total actions\n",
    "            transition_states = []\n",
    "           \n",
    "            #get the actions from all avaliable ones! \n",
    "            for act in avaliable_actions:\n",
    "                 #if we're at the current action\n",
    "                if(act == action):\n",
    "                    #append tuple pair of success and current action \n",
    "                    transition_states.append( (P_success_state, self.result(state, act)) )\n",
    "                #otherwise, carry on with the normal reverse induction step \n",
    "                else:\n",
    "                    transition_states.append( (P_others, self.result(state, act)))\n",
    "                    \n",
    "                \n",
    "            return transition_states  \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "default_reward = -0.01\n",
    "discount = 0.99\n",
    "terminal = {(25,9):100} #jus one goal state cause that's the only one that leaves\n",
    "mdp = MDP(nrows, ncols, terminal, default_reward, discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Part B:\n",
    "\n",
    "Using the MDP in part 2A, implement value iteration **or** policy iteration to calculate the utilities for each state. Also implement a function that takes as arguments an MDP object and a dictionary of state-utility pairs (key-value) and returns a dictionary for the optimal policy. The optimal policy dictionary should have state tuples as keys and the optimal move (None or any of the 8 directions) as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i kept trying to do policy iteration but it wasn't working right. \n",
    "\n",
    "def value_iteration(mdp, tol=1e-3):\n",
    "    utility_new = { s : 0 for s in mdp.states}\n",
    "    while True: \n",
    "        \n",
    "        utility_old = utility_new.copy()\n",
    "        max_change = 0\n",
    "        \n",
    "        #check for each and every state \n",
    "        for s in mdp.states: \n",
    "            next_states = [mdp.transition(s, act) for act in mdp.actions(s)]\n",
    "            \n",
    "            best_utility = -999; \n",
    "            \n",
    "            #calculate max espected utility! \n",
    "            for k in range(len(next_states)):\n",
    "                newsum = sum([next_states[k][j][0]*utility_old[next_states[k][j][1]] for j in range(len(next_states[k]))])\n",
    "                best_utility = max(best_utility, newsum)\n",
    "            \n",
    "            utility_new[s] = mdp.reward(s) + mdp.df * best_utility\n",
    "            \n",
    "            max_change = max(max_change, abs(utility_new[s]-utility_old[s]))\n",
    "        \n",
    "        # if maximum change in utility from one iteration to the\n",
    "        # next is less than some tolerance, break\n",
    "        if((mdp.df == 1 and max_change < 1) or (max_change < tol*(1-mdp.df)/mdp.df)):\n",
    "            break; \n",
    "    \n",
    "    return utility_new\n",
    "    \n",
    "    \n",
    "\n",
    "def find_policy(mdp, utility):\n",
    "    policy = {s : None for s in mdp.states}\n",
    "    \n",
    "    for s in mdp.states: \n",
    "        best_utility = (-999, None)\n",
    "        \n",
    "        for act in mdp.actions(s):\n",
    "            newsum = sum([p*utility[s2] for p, s2 in mdp.transition(s,act)])\n",
    "            \n",
    "            #if this action has a higher expected utility than current best, replace the best\n",
    "            #(utility, action) tuple with this one \n",
    "            \n",
    "            if (newsum > best_utility[0]):\n",
    "                best_utility = (newsum, act)\n",
    "              \n",
    "            \n",
    "        policy[s] = best_utility[1]\n",
    "    \n",
    "    return policy\n",
    "\n",
    "# result = policy_iteration(mdp)\n",
    "utility = value_iteration(mdp, 0.01)\n",
    "policies = find_policy(mdp, utility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Part C:\n",
    "\n",
    "Sanity check your answers in part B by listing which state has the *lowest* estimated utility (should be far from the goal!) and which states have the 3 *highest* estimated utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Utility at (1,1): \", utility.get(1,1))\n",
    "print(\"Utility at (25,10): \", utility.get(25,10))\n",
    "print(\"Utility at (25,8): \", utility.get(25,8))\n",
    "print(\"Utility at (24,9): \", utility.get(24,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Yup, these values make sense. This is because the states nearest the goal ought to have a high utility because we should be incentivized to go near them, while states farthest from the goal should have a lower utility because we should be incentivized to get away from them and go somewhere better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Part D:\n",
    "\n",
    "As in part 1E, make a figure.  This time, depict the optimal policy at each location.  Your choice of visualization is up to you, but I would *recommend* taking a plot similar to the maze plot in Part 1E above (or HW 2) and plotting some choice of arrows/symbols/colors corresponding to the appropriate action in each square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general numpy help with the grid function from: https://stackoverflow.com/questions/43971138/python-plotting-colored-grid-based-on-values\n",
    "\n",
    "\n",
    "#create new numpy array with zeros\n",
    "grid = np.zeros(shape=(nrows,ncols))\n",
    "\n",
    "#check if these are walls, convert them to 1\n",
    "for y in range(1, nrows):\n",
    "    for x in range(1, ncols):\n",
    "        if((x,y) in walls):\n",
    "            grid[nrows-y,x-1] = 1 \n",
    "\n",
    "\n",
    "#add last final bits of wall   \n",
    "grid[5, 24] = 1\n",
    "grid[4, 24] = 1\n",
    "grid[3, 24] = 1\n",
    "grid[2, 24] = 1\n",
    "grid[1, 24] = 1\n",
    "grid[0, 24] = 1\n",
    "grid[0, 23] = 1\n",
    "grid[0, 22] = 1\n",
    "grid[0, 21] = 1\n",
    "grid[0, 20] = 1\n",
    "\n",
    "    \n",
    "    \n",
    "#get goal space \n",
    "grid[nrows-9, 25-1] = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create discrete colormap\n",
    "cmap = colors.ListedColormap(['white', 'darkblue', 'orange'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,9))\n",
    "ax.imshow(grid, cmap=cmap)\n",
    "\n",
    "  # draw gridlines\n",
    "ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=1)\n",
    "ax.set_xticks(np.arange(0.51, 25, 1));\n",
    "ax.set_xticklabels([]);\n",
    "ax.set_yticks(np.arange(0.47, 16, 1));\n",
    "ax.set_yticklabels([]);\n",
    "\n",
    "#learned about arrows from: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.arrow.html\n",
    "for state in policies:\n",
    "    action = policies.get(state)\n",
    "    xst, yst = state\n",
    "    x = xst-1\n",
    "    y = nrows-yst\n",
    "    offset = 0.45\n",
    "\n",
    "    \n",
    "    if action=='N': \n",
    "        plt.arrow(x, y-offset-0.05+1, 0, -0.5,width=0.1, fc=\"magenta\", linewidth=0) #NORTH ARROW\n",
    "    elif action == 'NW':\n",
    "        plt.arrow(x+offset-0.05, y+offset-0.05, -0.5, -0.5,width=0.1, fc=\"magenta\", linewidth=0) #NORTHWEST ARROW\n",
    "    elif action=='S':\n",
    "        plt.arrow(x, y-offset, 0, 0.5,width=0.1, fc=\"magenta\", linewidth=0) #SOUTH ARROW\n",
    "    elif action=='SE':\n",
    "        plt.arrow(x-offset+0.05, y-offset, +0.5, +0.5,width=0.1, fc=\"magenta\", linewidth=0) #SOUTHEAST ARROW\n",
    "    elif action=='SW':\n",
    "        plt.arrow(x+offset-0.05, y-offset, -0.5, +0.5,width=0.1, fc=\"magenta\", linewidth=0) #SOUTHWEST ARROW\n",
    "    elif action=='E':\n",
    "        plt.arrow(x-offset, y, 0.5, 0,width=0.1, fc=\"magenta\", linewidth=0) #EAST ARROW\n",
    "    elif action=='W':\n",
    "        plt.arrow(1+offset, 1, -0.5, 0,width=0.1, fc=\"magenta\", linewidth=0) #WEST ARROW\n",
    "    elif action == 'NE':\n",
    "        plt.arrow(x-offset+0.05, y+offset-0.05, +0.5, -0.5,width=0.1, fc=\"magenta\", linewidth=0) #NORTHEAST ARROW\n",
    "\n",
    "\n",
    "\n",
    "# plt.arrow(1-offset, 1, 0.5, 0,width=0.1, facecolor=\"magenta\") #EAST\n",
    "# plt.arrow(0, 0-offset, 0, 0.5,width=0.1, facecolor=\"magenta\") #SOUTH\n",
    "# plt.arrow(2, 2-offset-0.05, 0, -0.5,width=0.1, facecolor=\"magenta\") #NORTH\n",
    "#plt.arrow(1+offset, 1, -0.5, 0,width=0.1, facecolor=\"magenta\") #WEST\n",
    "#plt.arrow(1+offset-0.05, 1+offset-0.05, -0.5, -0.5,width=0.1, facecolor=\"magenta\") #NORTHEAST\n",
    "#plt.arrow(1-offset+0.05, 1+offset-0.05, +0.5, -0.5,width=0.1, facecolor=\"magenta\") #NORTHWEST\n",
    "#plt.arrow(1+offset-0.05, 1-offset, -0.5, +0.5,width=0.1, facecolor=\"magenta\") #SOUTHWEST\n",
    "#plt.arrow(1-offset+0.05, 1-offset, +0.5, +0.5,width=0.1, facecolor=\"magenta\") #SOUTHEAST\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "<a id='p3'></a>\n",
    "\n",
    "\n",
    "---\n",
    "## [20 pts] Part 3:  The great unknown\n",
    "\n",
    "It's nearly winter solstice, and the area west of Engineering is still icy and slippery.  \n",
    "\n",
    "As a result, there's still a risk that we don't end up in the tile that we intend to move to!  Unfortunately, for this problem we *don't know* what that risk is, and it isn't the same everywhere.  As in Problem 2, our goal is to create a policy for an agent walking in the given space west of Engineering.  Again, the goal of the agent is to navigate from start to finish, but now we want a policy for each and every location on the map\n",
    "\n",
    "Suppose there exists some function $f$ that measures the *footing* of state $s$.  Then if we choose to take the action \"move towards state $s'$\" from state $n$, the probability that we *actually arrive* in the state $s'$ we intend to is $f(s)$.  As before, the remaining $1-f(s)$ probability is spread equally likely across all of the other adjacent (N/S/E/W/NW/NE/SE/SW) non-wall states to $s$.\n",
    "\n",
    "Because our agent can't fully observe the transitions, it's going to have to pick actions and estimate their utilities from learning.  Let's use Q-learning!\n",
    "\n",
    "#### Part A:\n",
    "\n",
    "Run the following code to provide a function for and a map of the footing function $f$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOU MAY IGNORE THIS CELL, BUT MUST RUN IT TO GENERATE F\n",
    "random.seed(30)\n",
    "x = np.linspace(0,25,26)\n",
    "y = np.linspace(0,25,26)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "f1 = np.zeros(X.shape)\n",
    "f2 = np.zeros(X.shape)\n",
    "f3 = np.zeros(X.shape)\n",
    "f4 = np.zeros(X.shape)\n",
    "\n",
    "mu1, mu2, mu3, mu4=[17,12],[17,11],[11,8],[11,6]\n",
    "covar1, covar2, covar3, covar4= [[16,8],[8,16]],[[12,.5],[.5,12]],[[4,.8],[.8,4]],[[.8,12],[.8,12]]\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        f1[i,j] = 6*stats.multivariate_normal.pdf(x=(X[i,j],Y[i,j]), mean=mu1, cov=covar1)\n",
    "        f2[i,j] = 3*stats.multivariate_normal.pdf(x=(X[i,j],Y[i,j]), mean=mu2, cov=covar2)\n",
    "        f3[i,j] = stats.multivariate_normal.pdf(x=(X[i,j],Y[i,j]), mean=mu3, cov=covar3)\n",
    "        f4[i,j] = 1*stats.multivariate_normal.pdf(x=(X[i,j],Y[i,j]), mean=mu4, cov=covar4)\n",
    "        \n",
    "f =f1+f2+f3+f4    \n",
    "f=1-(f/np.max(f))**(1/3)\n",
    "\n",
    "#PLOTTING:\n",
    "fig, ax = plt.subplots(1,1, figsize=(7,5))\n",
    "my_levels = np.linspace(0, 1, 11)\n",
    "labels = [str(lv) for lv in my_levels]\n",
    "cp = ax.contour(X, Y, f, levels=my_levels)\n",
    "plt.clabel(cp, inline=1, fontsize=10)\n",
    "ax.set(xlim=(0, 25), ylim=(0, 16))\n",
    "plt.title('Footing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can access f directly using indices if you use the TRANPOSE of the coordinates of each point\n",
    "#which this footing function does\n",
    "def footing(x,y):\n",
    "    return f[y,x]\n",
    "\n",
    "print(\"It's icy at (12,8), with almost no footing:\", footing(12,8))\n",
    "print(\"It's better at (8,12):\", footing(8,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B:\n",
    "\n",
    "We're going to implement Q-learning instead of an MDP.\n",
    "\n",
    "You will probably want to create a dictionary of the form discussed at the end of the in-class notebook for Q-learning, where each valid tuple is the first key and each valid move from that location is the second key.  You then should have the estimated utilities of each action saved in the resulting dictionary.  You may include other information if desired, but nothing else should be absolutely necessary.\n",
    "\n",
    "After initialization, print the elements of the dictionary corresponding to the (4,4) location.  Note that there should be 7 subdictionaries for the 6 possible neighbors and the `None` action, and within each action the initial Q-value should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacent_states_q(state):\n",
    "    #check 8 possible combinations of movement \n",
    "    x = state[0]\n",
    "    y = state[1]\n",
    "    neighbors = {} #empty dictionary to store neighboring values \n",
    "    \n",
    "    \n",
    "    #check if wall, failsafe \n",
    "    if((x,y) in walls):\n",
    "        return {};\n",
    "    \n",
    "    #North\n",
    "    if(y+1 <= nrows and (x, y+1) not in walls):\n",
    "        neighbors.update({\"N\": 0}) #append N, pathcost 1 \n",
    "    #NorthEast\n",
    "    if((x+1, y+1) <= (nrows, ncols) and (x+1, y+1) not in walls):\n",
    "        neighbors.update({\"NE\": 0}) #append NE, pathcost sqrt(2)\n",
    "    #East\n",
    "    if(x+1 <= ncols and (x+1, y) not in walls):\n",
    "        neighbors.update({\"E\": 0}) #append E, pathcost 1\n",
    "    #SouthEast\n",
    "    if(x+1 < ncols and y-1 > 0 and (x+1, y-1) not in walls):\n",
    "        neighbors.update({\"SE\": 0}) #append SE, pathcost sqrt(2)\n",
    "    #South\n",
    "    if(y-1 > 0 and (x, y-1) not in walls):\n",
    "        neighbors.update({\"S\": 0}) #append S, pathcost 1      \n",
    "    #SouthWest\n",
    "    if((x-1, y-1) > (0, 0) and (x-1, y-1) not in walls):\n",
    "        neighbors.update({\"SW\": 0}) #append SW, pathcost, pathcost sqrt(2)\n",
    "    #West\n",
    "    if(x-1 > 0 and (x-1, y) not in walls):\n",
    "        neighbors.update({\"W\": 0}) #append W, pathcost 1\n",
    "    #NorthWest\n",
    "    if(x-1 > 0 and y+1 <= nrows and (x-1, y+1) not in walls):\n",
    "        neighbors.update({\"NW\": 0}) #append NW, pathcost, pathcost sqrt(2)    \n",
    "    \n",
    "    return neighbors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create a Q, initialize all the Q-utilities as 0.\n",
    "qspaces = list(states)\n",
    "#remove walls (invalid spaces)\n",
    "for s in spaces:\n",
    "    if s in walls: \n",
    "        space.remove(s)\n",
    "        \n",
    "\n",
    "q_dict = {}\n",
    "utilities = [(0)] * len(states)\n",
    "for s in qspaces:\n",
    "    action_utility_dict = adjacent_states_q(s)\n",
    "    q_dict.update({(s): action_utility_dict})     \n",
    "    \n",
    "    \n",
    "\n",
    "print(q_dict[(1,1)]['E'])\n",
    "#print(q_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C:\n",
    "\n",
    "Use the given `footing` function to modify your `transitions` from the MDP so the probabilities of result given action now flow from the icy model above.\n",
    "\n",
    "Perform at least 1000 training epochs, where each starts at a *random* location from the valid states (this can help if find the goal state faster!).\n",
    "\n",
    "For each epoch, take *at most* 100 actions, or until the goal is reached.  You may choose these actions by any schema you desire, but I recommend the $\\varepsilon$-greedy agent that chooses the \"best available\" action 80% of the time and explores the other 20%. \n",
    "\n",
    "Show graphs depicting the actual paths taken for the last 2 of the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearn(start, goal, q_dict, qspaces, numtraining = 1000, numactions = 100):\n",
    "    gamma = 0.75 # Discount factor \n",
    "    alpha = 0.9 # Learning rate\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(numtraining):\n",
    "        #randomly select a state \n",
    "        rand_index = np.random.choice(len(qspaces)-1)\n",
    "        #retrieve tuple pair of state, along with corresponding actions \n",
    "        current_state = qspaces[rand_index]\n",
    "        current_actions_and_values = q_dict[current_state]\n",
    "\n",
    "        for i in range(numactions):\n",
    "            action_index = np.random.choice(len(current_actions_and_values)-1)\n",
    "            action = current_actions_and_values.keys()[action_index]\n",
    "            \n",
    "            #not entirely sure how to finish this thing....\n",
    "            \n",
    "            # Compute the Q-difference between where we started and where we ended up.\n",
    "            QD = rewards_new[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]\n",
    "\n",
    "            # Update the Q-Value using the Bellman equation\n",
    "            Q[current_state,next_state] += alpha * QD\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "qlearn(start, goal, q_dict, qspaces, 5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D:\n",
    "\n",
    "As in part 2D, make a figure depicting the optimal policy at each location.  Does your agent actually try to avoid the ice, compared to how it behaved in the MDP in Part 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
